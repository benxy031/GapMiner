sievePrototypeKernel (experimental CUDA sieve path)
=================================================

Overview
--------
The CUDA backend now includes a prototype sieve extractor that scans the CPU-built
bitset directly on the GPU. When the miner is launched with `--cuda-sieve-proto`
(and compiled with `USE_CUDA_BACKEND`), each sieve window is passed to CUDA to
enumerate candidate offsets before they are queued for Fermat testing.

Pipeline
--------
1. `HybridSieve::gpu_work_thread()` produces the sieve bitmap as before, but when
   the prototype flag is enabled it exports the bitmap and window metadata to
   `GPUFermat::prototype_sieve()` instead of scanning it on the CPU.
2. `GPUFermat::prototype_sieve_batch()` copies one or more consecutive windows
  to device memory, launches the sieve kernel once, and copies the batched
  offsets back to host memory before exposing per-window slices. Only windows
  backed by a fully materialized bitmap are included in these CUDA batches;
  residue-snapshot windows stick to the CPU scan path so mixed build modes
  never share a kernel launch.
3. The returned offsets are chunked into `GPUWorkItem`s exactly like CPU-generated
   candidates, so the rest of the pipeline (GPU Fermat tests, PoW submission) is
   unchanged.

Kernel Behavior (`src/CUDAFermat.cu`)
-------------------------------------
- Reads the sieve bitmap (one bit per candidate) and filters out composites.
- Ignores even offsets, since only odd candidates are useful for gap mining.
- Emits offsets relative to each sieve window (0 .. window_size) for
  surviving candidates; composites are marked with a `0xFFFFFFFF` sentinel.
  The host now re-adds the window base before scheduling Fermat work, which
  removes the previous 32-bit overflow limit on large searches.
- The host side compacts these offsets into a tightly packed vector before
  enqueuing them.

Current Scope
-------------
- The CPU still constructs the sieve bitmap (marking composites for each prime).
- The CUDA prototype only accelerates the bitmap scan / candidate enumeration.
- When the GPU queue is starved, the worker adaptively batches up to four sieve
  rounds per kernel launch, reducing host synchronization and copy overhead.
- Consecutive sieve windows reuse a shared bitmap buffer pool, so the GPU thread
  receives ownership of the ready bitmap without any extra host memcpy.
- Logging is visible via `-e` (extra verbose), e.g.,
  `CUDA sieve prototype window 9000000 start=... round=... candidates=362959`.
- Normal mining stats remain visible because the CPU threads still handle queue
  management and PoW validation.

Usage Notes
-----------
- Rebuild the CUDA miner (`make gapminer-cuda`) and run with
  `./bin/gapminer-cuda ... --cuda-sieve-proto`.
- `-w/--work-items` and `-z/--queue-size` continue to control queue depths; the
  prototype simply feeds them faster.
- If the flag is set without a CUDA build, a warning is printed and the code
  falls back to the traditional CPU sieve.

Adaptive Batching
-----------------
- The GPU worker opportunistically drains multiple ready `SieveItem`s from the
  queue (up to a hard cap of four) whenever the GPU candidate queue is sparsely
  populated.
- Batched windows share one bitmap/output transfer and one kernel launch while
  retaining per-window metadata so downstream code can continue operating on
  individual sieve rounds without any behavioral changes. Each batch’s slices
  are re-mapped back to their original `SieveItem` indices so logs can spell
  out exactly which CPU window contributed to a CUDA launch.
- Windows that rely on residue snapshots are processed individually to avoid
  mixing build modes in the same batch. Starting with the January 2026 tuning,
  the worker simply skips snapshot-backed windows when composing the batch and
  leaves them on the CPU scan path, which keeps the CUDA logs deterministic and
  prevents snapshot/bitmap artifacts from starving the GPU queue.

Future Work
-----------
- Move sieve bitmap construction to the GPU so the CPU no longer marks
  composites per window.
- Surface user-facing controls (CLI/env) for the prototype batch depth and add
  telemetry to tune batch sizing heuristics under diverse workloads.

Determinism, diagnostics and recent edits (Jan 2026)
-----------------------------------------------
- Per-window candidate ordering: the host-side pipeline now sorts per-window
  absolute offsets before assembling `GPUWorkItem` slices. This produces a
  deterministic ascending ordering of prototype candidates which aligns CUDA
  outputs with the OpenCL path and makes side-by-side comparisons reliable.

- Diagnostics routing: device-side candidate limb/carry dumps and per-limb
  carry traces are captured by the CUDA backend but written only to the
  extra-verbose log file (enabled with `-e`) to avoid noisy CLI output. Use
  the miner's extra-verbose file (named `tests` by default) to inspect these
  dumps.

- Submit & network correlation: submit instrumentation now logs full-precision
  `mpz_hash` and `mpz_adder` hex values, the computed `share_difficulty`, the
  `target`, and a human-friendly `ratio = share_difficulty / target`. RPC and
  Stratum payloads and raw responses are also recorded to the extra-verbose
  log so you can match miner submit attempts with pool/node replies.

- Developer tool: `tools/offset_difficulty` computes `PoW::difficulty()` for a
  provided `prime_base`, `target` and list of offsets. Use this to determinis-
  tically compare difficulties between CPU/OpenCL/CUDA for identical offsets.

These updates are diagnostic and ordering fixes only – they do not change the
core PoW arithmetic. They make debugging and parity testing between backends
much easier while keeping normal miner output uncluttered.

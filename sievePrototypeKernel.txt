sievePrototypeKernel (experimental CUDA sieve path)
=================================================

Overview
--------
The CUDA backend now includes a prototype sieve extractor that scans the CPU-built
bitset directly on the GPU. When the miner is launched with `--cuda-sieve-proto`
(and compiled with `USE_CUDA_BACKEND`), each sieve window is passed to CUDA to
enumerate candidate offsets before they are queued for Fermat testing.

Pipeline
--------
1. `HybridSieve::gpu_work_thread()` produces window metadata and (optionally)
  a CPU bitmap. When the prototype flag is enabled it exports metadata to
  `GPUFermat::prototype_sieve()` for GPU scanning.
2. `GPUFermat::prototype_sieve_batch()` copies one or more consecutive windows
  to device memory, builds bitmaps from residue snapshots when no CPU bitmap
  is provided, launches the scan kernel, and copies offsets back to host
  memory before exposing per-window slices. Bitmap-backed windows can be
  batched; residue-only windows are processed individually.
3. The returned offsets are chunked into `GPUWorkItem`s exactly like CPU-generated
  candidates, so the rest of the pipeline (GPU Fermat tests, PoW submission) is
  unchanged.

Kernel Behavior (`src/CUDAFermat.cu`)
-------------------------------------
- Reads the sieve bitmap (one bit per candidate) and filters out composites.
- Ignores even offsets, since only odd candidates are useful for gap mining.
- Emits offsets relative to each sieve window (0 .. window_size) for
  surviving candidates; composites are marked with a `0xFFFFFFFF` sentinel.
  The host now re-adds the window base before scheduling Fermat work, which
  removes the previous 32-bit overflow limit on large searches.
- The host side compacts these offsets into a tightly packed vector before
  enqueuing them.
- `locate_window()` function optimized with binary search (O(log n)) instead
  of linear search (O(n)) for better performance with large window counts.

Current Scope
-------------
- The CPU still computes residue snapshots and window metadata.
- The CUDA prototype builds bitmaps from residues when CPU bitmaps are absent,
  then scans the bitmap / enumerates candidates on-device.
- When the GPU queue is starved, the worker adaptively batches up to four sieve
  rounds per kernel launch, reducing host synchronization and copy overhead.
- Consecutive sieve windows reuse a shared bitmap buffer pool, so the GPU thread
  receives ownership of the ready bitmap without any extra host memcpy.
- Logging is visible via `-e` (extra verbose), e.g.,
  `CUDA sieve prototype window 9000000 start=... round=... candidates=362959`.
- Normal mining stats remain visible because the CPU threads still handle queue
  management and PoW validation.

Usage Notes
-----------
- Rebuild the CUDA miner (`make gapminer-cuda`) and run with
  `./bin/gapminer-cuda ... --cuda-sieve-proto`.
- `-w/--work-items` and `-z/--queue-size` continue to control queue depths; the
  prototype simply feeds them faster.
- If the flag is set without a CUDA build, a warning is printed and the code
  falls back to the traditional CPU sieve.

Adaptive Batching
-----------------
- The GPU worker opportunistically drains multiple ready `SieveItem`s from the
  queue (up to a hard cap of four) whenever the GPU candidate queue is sparsely
  populated.
- Batched windows share one bitmap/output transfer and one kernel launch while
  retaining per-window metadata so downstream code can continue operating on
  individual sieve rounds without any behavioral changes. Each batch’s slices
  are re-mapped back to their original `SieveItem` indices so logs can spell
  out exactly which CPU window contributed to a CUDA launch.
- Windows that rely on residue snapshots are processed individually to avoid
  mixing build modes in the same batch. Snapshot windows are now fully GPU
  handled; CPU scan fallback is skipped for residue-only windows.

Future Work
-----------
- Move sieve bitmap construction to the GPU so the CPU no longer marks
  composites per window.
- Surface user-facing controls (CLI/env) for the prototype batch depth and add
  telemetry to tune batch sizing heuristics under diverse workloads.

Determinism, diagnostics and recent edits (Jan 2026)
-----------------------------------------------
- Per-window candidate ordering: the host-side pipeline now sorts per-window
  absolute offsets before assembling `GPUWorkItem` slices. This produces a
  deterministic ascending ordering of prototype candidates which aligns CUDA
  outputs with the OpenCL path and makes side-by-side comparisons reliable.

- Diagnostics routing: device-side candidate limb/carry dumps and per-limb
  carry traces are captured by the CUDA backend but written only to the
  extra-verbose log file (enabled with `-e`) to avoid noisy CLI output. Use
  the miner's extra-verbose file (named `tests` by default) to inspect these
  dumps.

- Submit & network correlation: submit instrumentation now logs full-precision
  `mpz_hash` and `mpz_adder` hex values, the computed `share_difficulty`, the
  `target`, and a human-friendly `ratio = share_difficulty / target`. RPC and
  Stratum payloads and raw responses are also recorded to the extra-verbose
  log so you can match miner submit attempts with pool/node replies.

- Developer tool: `tools/offset_difficulty` computes `PoW::difficulty()` for a
  provided `prime_base`, `target` and list of offsets. Use this to determinis-
  tically compare difficulties between CPU/OpenCL/CUDA for identical offsets.

- Kernel optimizations (Jan 2026): `locate_window()` function optimized with
  binary search (O(log n)) instead of linear search (O(n)) for better performance
  with large window counts. `sievePrototypeScanKernel` reverted to atomicAdd-based
  implementation for correctness and stability, avoiding shared memory layout
  bugs that caused illegal memory access and reduced batching efficiency.

These updates are diagnostic and ordering fixes only – they do not change the
core PoW arithmetic. They make debugging and parity testing between backends
much easier while keeping normal miner output uncluttered.

Runtime / stream model changes (Jan 2026)
-----------------------------------------
- Dynamic CUDA streams: `GPUFermat` now allocates a configurable number of
  CUDA streams instead of a fixed two-element array. The constructor and
  `get_instance()` gained an optional `streamCount` parameter (default 2)
  so callers can request more or fewer streams at creation time.
- `GPUFermat::get_stream_count()` was added to read the active stream count
  at runtime (useful for diagnostics and telemetry).
- Extra-verbose stream handles: when extra-verbose logging (`-e`) is enabled,
  the CUDA initializer prints the number of streams and their opaque handles
  (e.g. `CUDA streams count=2 handles: [0]=0x5d815dead740 [1]=0x5d815dff5b10`).
  Those hex values are internal `cudaStream_t` identifiers (opaque handles)
  and confirm successful stream creation — they are not addresses you should
  dereference or perform arithmetic on.
- Impact on the pipeline: `run_cuda()` round-robins chunks across the active
  streams (`streams[chunkIndex % streamCount]`) so increasing `streamCount`
  can increase host→device / kernel concurrency and the effective GPU queue
  depth. Conversely, a lower `streamCount` reduces outstanding async ops and
  memory pressure.

How to use / adjust
--------------------
- To request a different stream count, modify the callsite that creates the
  singleton (normally `GPUFermat::get_instance(...)` in `src/main.cpp`) and
  pass the desired `streamCount`. The constructor default remains 2 for
  backward compatibility.
- Alternatively, read `get_stream_count()` to log or assert runtime
  configuration without altering construction callsites.

These changes are low-risk and focused on orchestration (streams and logging)
— they do not change kernel logic or candidate semantics.

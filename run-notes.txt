2026-01-22 CUDA Sieve Kernel Optimizations
---------------------------------------
- `locate_window()` function in `src/CUDAFermat.cu` optimized with binary search
  (O(log n)) instead of linear search (O(n)) for better performance with large
  window counts. This improves efficiency when processing many sieve windows.
- `sievePrototypeScanKernel` reverted to atomicAdd-based implementation for
  correctness and stability. The previous shared memory parallel scan version
  introduced illegal memory access bugs and reduced batching efficiency. The
  atomic implementation maintains thread safety while avoiding complex shared
  memory layout issues.
- These changes prioritize correctness and performance over experimental
  optimizations that introduced instability.

2026-01-26 Memory and Transfer Optimizations
--------------------------------------------
- Pinned memory implemented for all GPU buffers in `CudaBuffer::init()` with
  `prefer_pinned = true`. This enables faster PCIe transfers by using
  `cudaHostAlloc` for host data, falling back to pageable memory if allocation
  fails. Applied to sieve bitmaps, candidate outputs, and Fermat operands.
- Sparse candidate representation: GPU sieve prototype outputs candidate offsets
  as a compact list of uint32_t indices instead of a full bitmap, reducing
  downstream processing overhead and enabling efficient batching.
- These optimizations reduce RAM usage (e.g., RTX 3060 from 25GB to 7GB) and
  improve transfer performance for the experimental CUDA sieve prototype.

2026-01-26 RTX 3060 Optimized Config
------------------------------------
Tuned configuration for NVIDIA RTX 3060 with --cuda-sieve-proto:
- Command: `./gapminer-cuda -o 127.0.0.1 -p 31397 -u user -x pass --use-gpu -a nvidia -w 8192 -n 96 --cuda-sieve-proto --sieve-size 20000000 --sieve-primes 5000000 --queue-size 65536 --bitmap-pool-buffers 2048 --snapshot-pool-buffers 64 --gpu-launch-divisor 24 --gpu-launch-wait-ms 100 -e`
- Performance: ~1.1M pps, ~4.8M tests/s, stable GPU batches (avg 96 tests), ~7GB RAM usage.
- Key changes: Increased sieve params for faster CPU production, scaled buffer pools to reduce RAM from 25GB to 7GB, adjusted launch params for better batching.
- Note: Standard CUDA (without --cuda-sieve-proto) may offer higher throughput (~2.2M pps), but proto provides experimental GPU sieve offloading with optimized tuning.

2026-01-26 Configurable GPU Shift and Miller-Rabin Optimization
---------------------------------------------------------------
- GPU shift made configurable via `-f/--shift` option. Defaults to 45 for GPU
  (20 for CPU) if not specified, allowing experimentation with different shift
  values for performance tuning.
- Miller-Rabin primality test rounds reduced from 45 to 25 in verification and
  logging code. Provides ~44% faster checks with negligible false positive risk
  (< 2^-50 error probability).
- Tested with `-f 50`: Successfully found share with ratio 1.132816, confirming
  configurable shift works correctly for custom values.

2026-01-14 GPU sieve notes
--------------------------
Run configuration: `-w 10240 -s 110000000 --cuda-sieve-proto`
- Large work queue (10,240 items) keeps up to ~5.2M candidate offsets staged
  when `--num-gpu-tests` stays at 8.
- Sieve size 110M means each `SieveItem` window covers 110 million offsets;
  copying that bitmap to CUDA per window costs ~14 MB per transfer, so overall
  PCIe traffic increases. Expect a trade-off: larger windows produce more
  candidates per kernel launch but take longer for the CPU sieve thread to
  prepare and copy.
- If `GPU queue depth` logs still drop near zero, consider bumping
  `--num-gpu-tests` or `--queue-size` so the GPU isn’t starved while the CPU
  finishes the next window.

2026-01-15 Fixed GPU batch sizing
---------------------------------
Removed the adaptive controller (`GPUWorkList::adjust_active_tests`) because it
oscillated between 750/1000 tests and starved the queue. `GPU queue depth X/Y
items using Z tests` now always reports the fixed batch size set by
`-w/--work-items` (or the compiled default). Adjust `-w` directly when tuning;
`-n/--num-gpu-tests` remains the count of Fermat iterations per offset bundle.
If the queue still drains, raise `-w` or lighten sieve filtering instead of
expecting the runtime to resize batches.

2026-01-15 Batch prefilling
---------------------------
`GPUWorkList::create_candidates()` now waits up to ~5 ms for the queue to reach
roughly 1/6 of its capacity before launching the next kernel. This avoids the
"queue depth 1/XXXX" partial batches that used to leave the GPU idle and keeps
each launch closer to the configured `-w` value. If the queue can’t reach that
threshold (e.g. while draining during a block change), the wait expires and the
GPU still processes whatever is available, so overall latency stays bounded.

2026-01-15 GPU queue instrumentation
------------------------------------
`GPU-Items: XXX MB  avg: YYY tests  min: ZZZ tests` now reflects the queue just
before each Fermat launch instead of the empty queue afterward. Stats ignore
drained `GPUWorkItem`s, so `avg/min` only describe items with remaining tests.
Expect these lines to pause briefly whenever batch prefilling waits for more
work; that simply means the queue is being topped up so the GPU launches at the
requested `-w/--work-items` size.

2026-01-15 Adaptive CUDA sieve windows
-------------------------------------
- `gpu_work_thread` now groups multiple `SieveItem`s into a single CUDA launch
  whenever the GPU queue is under ~60% full. The heuristic tops out at four
  windows per launch so bitmap copies stay bounded while still amortizing PCIe
  traffic.
- `GPUFermat::prototype_sieve_batch()` reuses one bitmap/output buffer across
  the group and records prefix offsets so each window’s Fermat offsets remain
  identifiable downstream.
- Windows that carry residue snapshots still run solo; mixing snapshot and
  bitmap builds in the same kernel complicated synchronization and didn’t help
  throughput in testing.
- Extra verbose logs will emit `CUDA sieve prototype batched N windows; ...`
  whenever batching occurs. If you never see batched windows, raise
  `--work-items` or `--queue-size` so the heuristic has headroom to pull a few
  extra items off the sieve queue before launching.

2026-01-15 Bitmap-only CUDA batches
-----------------------------------
- After instrumenting the GPU prototype we now restrict CUDA batching to
  windows that already exported a bitmap. Residue-snapshot windows bypass the
  prototype code entirely and stick with the legacy CPU scan, which keeps each
  CUDA launch homogeneous and prevents the compact-scan fallbacks that happened
  when mixed window types rode the same batch.
- Because the worker tracks which batch entry maps back to each `SieveItem`,
  the host can deterministically log `CUDA sieve prototype batched N windows;`
  or `CUDA sieve prototype compact scan fallback ...` for every bitmap-backed
  launch. Snapshot windows still enqueue candidates; they just do so outside
  the prototype path, so Fermat throughput is unchanged.
- `--num-gpu-tests` continues to define how many sieve offsets the GPU results
  thread drains before each `fermat_gpu()` call. Pair it with a larger
  `--work-items` value when you want deeper queues and higher launch occupancy;
  when the queue dips under roughly 3k items the prefilling stage will wait up
  to ~50 ms before running a partial batch.
2026-02-06 CUDA Comba Montgomery Multiplication
-----------------------------------------------
- Implemented Comba/CIOS Montgomery multiplication variant in `src/CUDAFermat.cu`
  as an alternative to the classic unrolled implementation.
- Comba variant achieves ~300 Mmul/s on RTX 3060 (vs. ~295 Mmul/s unrolled) with
  reduced register pressure (56 vs. 58 regs), improving GPU occupancy.
- Runtime flag `--cuda-comba` enables the Comba path in both GPU benchmark and
  live CUDA mining (independent of `--cuda-sieve-proto`).
- Configuration is passed to device via constant memory (`kUseCombaConst`) in
  `GPUFermat::init_cuda()`. Startup config log displays `cuda_comba=on/off`.
- Benchmark comparison: Classic (0.490 ms, 268 Mmul/s) < Unrolled (0.445 ms, 295 Mmul/s) ≤ Comba (0.436 ms, 300 Mmul/s).
- Use `--cuda-comba` when targeting lower register pressure or testing higher
  GPU occupancy; standard unrolled remains default for compatibility.
- Example: `./gapminer-cuda -o host -p port -u user -x pass --use-gpu -a nvidia -f 43 -w 11264 -n 60 --cuda-comba [other options]`

2026-02-07 CUDA sieve prototype GPU-only residue path
-----------------------------------------------------
- Residue-only windows now build the sieve bitmap on the GPU and skip CPU
  bitmap scans entirely. CPU bitmaps are only built when present.
- Batching still groups bitmap-backed windows, while residue-only windows run
  as single-window launches to keep kernels homogeneous.
- Prototype logging adds an explicit `GPU sieve offsets produced: yes` line in
  extra-verbose mode (`-e`) after offsets are generated.
- Residue-only batches skip the legacy scan path if compact scan fails, so
  candidate enumeration remains GPU-only.

2026-02-07 GPU results thread CPU yielding optimization
-------------------------------------------------------
- Added explicit `sched_yield()` after `parse_results()` in GPU results thread
  to improve work thread scheduling when `-e` flag is not used.
- The `-e` flag's file I/O syscalls naturally cause kernel context switches
  that yield CPU to other threads, providing better scheduling than default.
  This optimization replicates that behavior artificially.
- Performance improvement: ~3-4% throughput gain without `-e` flag.
  - Original: ~906k-919k PPS without `-e`, ~2.15M PPS with `-e` (2.4x gap)
  - Optimized: ~937k-947k PPS without `-e`, ~2.15M PPS with `-e` (2.25x gap)
- For production use with large queue sizes, `-e` flag still recommended for
  maximum performance (2.1-2.2M PPS range), as file I/O provides more
  significant scheduling benefit than single sched_yield().

2026-02-07 GPU batch wait thread CPU yielding optimization
----------------------------------------------------------
- Root cause identified: Without `-e` flag, the GPU batch wait loop
  (`GPUWorkList::create_candidates()`) runs tight without yielding CPU,
  causing thread starvation when both GPU work and results threads compete.
  The `-e` flag's file I/O naturally blocks and yields to other threads.
- Fix: Added explicit `sched_yield() + usleep(100)` during batch wait
  (timedwait loop) to mimic `-e` flag's blocking behavior. Results thread
  now releases mutex and yields CPU while waiting for queue to prefill,
  allowing work thread to populate queue faster without requiring file I/O.
- Expected improvement: ~2.4x throughput gain without `-e` flag when using
  large queue sizes (32768+) and launch divisors (24+). Performance should
  now match `-e` mode without the logging overhead.
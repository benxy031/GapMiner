2026-01-22 CUDA Sieve Kernel Optimizations
---------------------------------------
- `locate_window()` function in `src/CUDAFermat.cu` optimized with binary search
  (O(log n)) instead of linear search (O(n)) for better performance with large
  window counts. This improves efficiency when processing many sieve windows.
- `sievePrototypeScanKernel` reverted to atomicAdd-based implementation for
  correctness and stability. The previous shared memory parallel scan version
  introduced illegal memory access bugs and reduced batching efficiency. The
  atomic implementation maintains thread safety while avoiding complex shared
  memory layout issues.
- These changes prioritize correctness and performance over experimental
  optimizations that introduced instability.

2026-01-26 Memory and Transfer Optimizations
--------------------------------------------
- Pinned memory implemented for all GPU buffers in `CudaBuffer::init()` with
  `prefer_pinned = true`. This enables faster PCIe transfers by using
  `cudaHostAlloc` for host data, falling back to pageable memory if allocation
  fails. Applied to sieve bitmaps, candidate outputs, and Fermat operands.
- Sparse candidate representation: GPU sieve prototype outputs candidate offsets
  as a compact list of uint32_t indices instead of a full bitmap, reducing
  downstream processing overhead and enabling efficient batching.
- These optimizations reduce RAM usage (e.g., RTX 3060 from 25GB to 7GB) and
  improve transfer performance for the experimental CUDA sieve prototype.

2026-01-26 RTX 3060 Optimized Config
------------------------------------
Tuned configuration for NVIDIA RTX 3060 with --cuda-sieve-proto:
- Command: `./gapminer-cuda -o 127.0.0.1 -p 31397 -u user -x pass --use-gpu -a nvidia -w 8192 -n 96 --cuda-sieve-proto --sieve-size 20000000 --sieve-primes 5000000 --queue-size 65536 --bitmap-pool-buffers 2048 --snapshot-pool-buffers 64 --gpu-launch-divisor 24 --gpu-launch-wait-ms 100 --min-gap 1000 -e`
- Performance: ~1.1M pps, ~4.8M tests/s, stable GPU batches (avg 96 tests), ~7GB RAM usage.
- Key changes: Increased sieve params for faster CPU production, scaled buffer pools to reduce RAM from 25GB to 7GB, adjusted launch params for better batching.
- Note: Standard CUDA (without --cuda-sieve-proto) may offer higher throughput (~2.2M pps), but proto provides experimental GPU sieve offloading with optimized tuning.

2026-01-26 Configurable GPU Shift and Miller-Rabin Optimization
---------------------------------------------------------------
- GPU shift made configurable via `-f/--shift` option. Defaults to 45 for GPU
  (20 for CPU) if not specified, allowing experimentation with different shift
  values for performance tuning.
- Miller-Rabin primality test rounds reduced from 45 to 25 in verification and
  logging code. Provides ~44% faster checks with negligible false positive risk
  (< 2^-50 error probability).
- Tested with `-f 50`: Successfully found share with ratio 1.132816, confirming
  configurable shift works correctly for custom values.

2026-01-14 GPU sieve notes
--------------------------
Run configuration: `-w 10240 -s 110000000 --cuda-sieve-proto`
- Large work queue (10,240 items) keeps up to ~5.2M candidate offsets staged
  when `--num-gpu-tests` stays at 8.
- Sieve size 110M means each `SieveItem` window covers 110 million offsets;
  copying that bitmap to CUDA per window costs ~14 MB per transfer, so overall
  PCIe traffic increases. Expect a trade-off: larger windows produce more
  candidates per kernel launch but take longer for the CPU sieve thread to
  prepare and copy.
- If `GPU queue depth` logs still drop near zero, consider bumping
  `--num-gpu-tests` or `--queue-size` so the GPU isn’t starved while the CPU
  finishes the next window.

2026-01-15 Fixed GPU batch sizing
---------------------------------
Removed the adaptive controller (`GPUWorkList::adjust_active_tests`) because it
oscillated between 750/1000 tests and starved the queue. `GPU queue depth X/Y
items using Z tests` now always reports the fixed batch size set by
`-w/--work-items` (or the compiled default). Adjust `-w` directly when tuning;
`-n/--num-gpu-tests` remains the count of Fermat iterations per offset bundle.
If the queue still drains, raise `-w` or lighten sieve filtering instead of
expecting the runtime to resize batches.

2026-01-15 Batch prefilling
---------------------------
`GPUWorkList::create_candidates()` now waits up to ~5 ms for the queue to reach
roughly 1/6 of its capacity before launching the next kernel. This avoids the
"queue depth 1/XXXX" partial batches that used to leave the GPU idle and keeps
each launch closer to the configured `-w` value. If the queue can’t reach that
threshold (e.g. while draining during a block change), the wait expires and the
GPU still processes whatever is available, so overall latency stays bounded.

2026-01-15 GPU queue instrumentation
------------------------------------
`GPU-Items: XXX MB  avg: YYY tests  min: ZZZ tests` now reflects the queue just
before each Fermat launch instead of the empty queue afterward. Stats ignore
drained `GPUWorkItem`s, so `avg/min` only describe items with remaining tests.
Expect these lines to pause briefly whenever batch prefilling waits for more
work; that simply means the queue is being topped up so the GPU launches at the
requested `-w/--work-items` size.

2026-01-15 Adaptive CUDA sieve windows
-------------------------------------
- `gpu_work_thread` now groups multiple `SieveItem`s into a single CUDA launch
  whenever the GPU queue is under ~60% full. The heuristic tops out at four
  windows per launch so bitmap copies stay bounded while still amortizing PCIe
  traffic.
- `GPUFermat::prototype_sieve_batch()` reuses one bitmap/output buffer across
  the group and records prefix offsets so each window’s Fermat offsets remain
  identifiable downstream.
- Windows that carry residue snapshots still run solo; mixing snapshot and
  bitmap builds in the same kernel complicated synchronization and didn’t help
  throughput in testing.
- Extra verbose logs will emit `CUDA sieve prototype batched N windows; ...`
  whenever batching occurs. If you never see batched windows, raise
  `--work-items` or `--queue-size` so the heuristic has headroom to pull a few
  extra items off the sieve queue before launching.

2026-01-15 Bitmap-only CUDA batches
-----------------------------------
- After instrumenting the GPU prototype we now restrict CUDA batching to
  windows that already exported a bitmap. Residue-snapshot windows bypass the
  prototype code entirely and stick with the legacy CPU scan, which keeps each
  CUDA launch homogeneous and prevents the compact-scan fallbacks that happened
  when mixed window types rode the same batch.
- Because the worker tracks which batch entry maps back to each `SieveItem`,
  the host can deterministically log `CUDA sieve prototype batched N windows;`
  or `CUDA sieve prototype compact scan fallback ...` for every bitmap-backed
  launch. Snapshot windows still enqueue candidates; they just do so outside
  the prototype path, so Fermat throughput is unchanged.
- `--num-gpu-tests` continues to define how many sieve offsets the GPU results
  thread drains before each `fermat_gpu()` call. Pair it with a larger
  `--work-items` value when you want deeper queues and higher launch occupancy;
  when the queue dips under roughly 3k items the prefilling stage will wait up
  to ~50 ms before running a partial batch.
